data_url = paste0(
'https://archive.ics.uci.edu/ml/',
'machine-learning-databases/wine-quality/',
'winequality-red.csv')
data = read.csv(url(data_url), sep = ';')
head(data)
dim(data)
data_url = paste0(
'https://archive.ics.uci.edu/ml/',
'machine-learning-databases/wine-quality/',
'winequality-red.csv')
summary(data)
if(!require(caret)) install.packages("caret"); library(caret)
if(!require(tidyselect)) install.packages("tidyselect"); library(tidyselect)
pairs(data)
boxplot(data)
hist(data$quality)
set.seed(1234)
idx = createDataPartition(data$quality, p=.7, list=F)
data_train = data[idx, ]
data_test  = data[-idx, ]
dim(data_train)
dim(data_test)
( dim(data_train)[1] / (dim(data_train)[1] + dim(data_test)[1]) )*100
knn = train(
quality ~ .,             # 예측하고자 하는 y 값는 quality라는 열 !
data=data_train,
method='knn',
tuneGrid=data.frame(.k = 3))
knn
y_train_pred = predict(knn, data_train)
hist(y_train_pred)
plot(data_train$quality,y_train_pred)
abline(a=0,b=1,col="black",lty=6)
RMSE(data_train$quality, y_train_pred)
y_pred = predict(knn, data_test)
plot(data_test$quality,y_pred)
abline(a=0,b=1,col="black",lty=6)
RMSE(data_test$quality, y_pred)
knn_3 = train(
quality ~ .,
data=data_train,
method='knn',
preProc = c('center', 'scale'),
tuneGrid=data.frame(.k = seq(1, 50, 5)))
knn_3
y_train_pred = predict(knn_3, data_train)
plot(data_train$quality,y_train_pred)
abline(a=0,b=1,col="black",lty=6)
RMSE(data_train$quality, y_train_pred)
y_pred = predict(knn_3, data_test)
plot(data_test$quality,y_pred)
abline(a=0,b=1,col="black",lty=6)
RMSE(data_test$quality, y_pred)
require("class")
group_1 <- cut(seq(1,533),breaks=5,labels=FALSE)
group_2 <- cut(seq(534,1066),breaks=5,labels=FALSE)
group_3 <- cut(seq(1067,1599),breaks=5,labels=FALSE)
fold <- c(group_1, group_2, group_3)
acc <- c() # accuracy for each fold
for (i in 1:5){
ds.tr <- data[fold != i, 1:11]
ds.ts <- data[fold == i, 1:11]
cl.tr <- data[fold != i, 12]
cl.ts <- data[fold == i, 12]
pred <- knn(ds.tr, ds.ts, cl.tr, k = 3)
acc[i] <- mean(pred==cl.ts) # 예측 정확도
}
acc
mean(acc)
?knn
data_url = paste0(
'https://archive.ics.uci.edu/ml/',
'machine-learning-databases/wine-quality/',
'winequality-red.csv')
data = read.csv(url(data_url), sep = ';')
head(data)
dim(data)
summary(data)
if(!require(caret)) install.packages("caret"); library(caret)
if(!require(tidyselect)) install.packages("tidyselect"); library(tidyselect)
pairs(data)
boxplot(data)
hist(data$quality)
set.seed(1234) # 난수고정
idx = createDataPartition(data$quality, p=.7, list=F) # 층화추출 , y인 열의 빈도를 기준으로 train 0.7 , test 0.3 의 비율로 구분하겠다.
data_train = data[idx, ]
data_test  = data[-idx, ]
dim(data_train)
dim(data_test)
knn = train(
quality ~ .,             # 예측하고자 하는 y 값는 quality라는 열 !
data=data_train,
method='knn',
tuneGrid=data.frame(.k = 3))
y_train_pred = predict(knn, data_train)
hist(y_train_pred)
plot(data_train$quality,y_train_pred)
abline(a=0,b=1,col="black",lty=6)
RMSE(data_train$quality, y_train_pred)
y_pred = predict(knn, data_test)
plot(data_test$quality,y_pred)
abline(a=0,b=1,col="black",lty=6)
RMSE(data_test$quality, y_pred)
plot(data_train$quality,y_train_pred)
abline(a=0,b=1,col="black",lty=6)
RMSE(data_train$quality, y_train_pred)
knn_3 = train(
quality ~ .,
data=data_train,
method='knn',
preProc = c('center', 'scale'),
tuneGrid=data.frame(.k = seq(1, 50, 5)))
knn_3
y_train_pred = predict(knn_3, data_train)
plot(data_train$quality,y_train_pred)
abline(a=0,b=1,col="black",lty=6)
RMSE(data_train$quality, y_train_pred)
y_pred = predict(knn_3, data_test)
plot(data_test$quality,y_pred)
abline(a=0,b=1,col="black",lty=6)
RMSE(data_test$quality, y_pred)
data_url = paste0(
'https://archive.ics.uci.edu/ml/',
'machine-learning-databases/wine-quality/',
'winequality-red.csv')
data = read.csv(url(data_url), sep = ';')
if(!require(caret)) install.packages("caret"); library(caret)
if(!require(tidyselect)) install.packages("tidyselect"); library(tidyselect)
set.seed(1234) # 난수고정
idx = createDataPartition(data$quality, p=.7, list=F) # 층화추출 , y인 열의 빈도를 기준으로 train 0.7 , test 0.3 의 비율로 구분하겠다.
data_train = data[idx, ]
data_test  = data[-idx, ]
dim(data_train)
dim(data_test)
( dim(data_train)[1] / (dim(data_train)[1] + dim(data_test)[1]) )*100 # 7:3의 비율로 나뉜 것을 알 수 있음.
knn = train(
quality ~ .,             # 예측하고자 하는 y 값는 quality라는 열 !
data=data_train,
method='knn',
tuneGrid=data.frame(.k = 3))
knn
y_train_pred = predict(knn, data_train)
hist(y_train_pred)
plot(data_train$quality,y_train_pred)
abline(a=0,b=1,col="black",lty=6)
RMSE(data_train$quality, y_train_pred)
RMSE(data_test$quality, y_pred)
y_pred = predict(knn, data_test)
RMSE(data_test$quality, y_pred)
knn_3 = train(
quality ~ .,
data=data_train,
method='knn',
preProc = c('center', 'scale'),
tuneGrid=data.frame(.k = seq(1, 50, 5)))
knn_3
y_train_pred = predict(knn_3, data_train)
RMSE(data_train$quality, y_train_pred)
y_pred = predict(knn_3, data_test)
RMSE(data_test$quality, y_pred)
require("class")
group_1 <- cut(seq(1,533),breaks=5,labels=FALSE)
group_2 <- cut(seq(534,1066),breaks=5,labels=FALSE)
group_3 <- cut(seq(1067,1599),breaks=5,labels=FALSE)
fold <- c(group_1, group_2, group_3)
acc <- c() # accuracy for each fold
for (i in 1:5){
ds.tr <- data[fold != i, 1:11]
ds.ts <- data[fold == i, 1:11]
cl.tr <- data[fold != i, 12]
cl.ts <- data[fold == i, 12]
pred <- knn(ds.tr, ds.ts, cl.tr, k = 3)
acc[i] <- mean(pred==cl.ts) # 예측 정확도
}
acc
